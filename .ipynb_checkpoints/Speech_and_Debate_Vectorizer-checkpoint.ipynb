{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['presidential_speech', 'clean', 'presidential_debates']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "conn = 'mongodb://timanderin.info:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "speechDB = client.speech_db\n",
    "debatesCOL = speechDB.presidential_debates\n",
    "speeches_COL = speechDB.presidential_speech\n",
    "\n",
    "clean = client.speech_db.clean\n",
    "\n",
    "speechDB.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "\n",
    "debates_coll = debatesCOL.find()\n",
    "\n",
    "#Pulling the first speech for testing purposes.\n",
    "speech = debates_coll[0]['text']\n",
    "\n",
    "#Stripping punctuation.\n",
    "no_punc = speech.translate(str.maketrans('', '', string.punctuation))\n",
    "no_punc\n",
    "\n",
    "#Will append all speeches and debates to list.\n",
    "decades_list = ['what type', 'is this', 'kind of thing', 'kind']\n",
    "decades_list.append(no_punc)\n",
    "len(decades_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2109,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count vectorizer - retrieving words count frequencies.\n",
    "vectorizer=CountVectorizer(stop_words= 'english', lowercase=False)\n",
    "X = vectorizer.fit_transform(decades_list)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "#Matrix of word counts for each debate or speech entry. Needs reshaping so word count and vector array are same size.\n",
    "matrix = X.toarray()\n",
    "counts = matrix.sum(axis=0)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>And</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>TRUMP</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>We</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>HOLT</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>going</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>But</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>CLINTON</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>think</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>ve</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>country</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>people</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>You</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>just</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>It</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>know</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>said</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>want</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Clinton</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Well</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Secretary</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>look</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>So</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>The</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>don</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>That</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>say</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>jobs</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>They</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>really</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Trump</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>illegally</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>ideas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Words</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>hurtful</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>huge</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>housing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>hours</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>hostages</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>hospitals</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>horribly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>abused</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>implicitly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>impose</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>improve</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>infant</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>inequality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>industry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>industries</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>independent</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>increasingly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Yellen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>inconvenience</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>inclusive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>incident</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>inchief</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>incarcerated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>inappropriate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2109 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Count\n",
       "75              And    220\n",
       "350           TRUMP    124\n",
       "391              We     98\n",
       "174            HOLT     97\n",
       "1063          going     90\n",
       "103             But     89\n",
       "105         CLINTON     87\n",
       "1930          think     79\n",
       "2019             ve     73\n",
       "767         country     72\n",
       "1486         people     71\n",
       "413             You     64\n",
       "1235           just     63\n",
       "206              It     61\n",
       "1246           know     61\n",
       "1720           said     58\n",
       "2041           want     51\n",
       "117         Clinton     50\n",
       "392            Well     48\n",
       "329       Secretary     48\n",
       "1311           look     48\n",
       "337              So     47\n",
       "355             The     46\n",
       "886             don     45\n",
       "354            That     44\n",
       "1730            say     43\n",
       "1230           jobs     42\n",
       "359            They     40\n",
       "1633         really     39\n",
       "372           Trump     37\n",
       "...             ...    ...\n",
       "1163      illegally      1\n",
       "1160          ideas      1\n",
       "405           Words      1\n",
       "1157        hurtful      1\n",
       "1155           huge      1\n",
       "1154        housing      1\n",
       "1153          hours      1\n",
       "1151       hostages      1\n",
       "1150      hospitals      1\n",
       "1149       horribly      1\n",
       "419          abused      1\n",
       "1171     implicitly      1\n",
       "1173         impose      1\n",
       "1174        improve      1\n",
       "1196         infant      1\n",
       "1195     inequality      1\n",
       "1193       industry      1\n",
       "1192     industries      1\n",
       "1191    independent      1\n",
       "1189   increasingly      1\n",
       "409          Yellen      1\n",
       "1185  inconvenience      1\n",
       "410           Yemen      1\n",
       "411             Yes      1\n",
       "1182      inclusive      1\n",
       "1179       incident      1\n",
       "1178        inchief      1\n",
       "1176   incarcerated      1\n",
       "1175  inappropriate      1\n",
       "0                04      1\n",
       "\n",
       "[2109 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Words and Counts dataframe.\n",
    "df = pd.DataFrame({'Word':words, 'Count':counts})\n",
    "df.sort_values(by='Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "words_2010_count = clean.count_documents({'year':{'$gte': 2009, '$lte':2019}})\n",
    "\n",
    "print(words_2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_2010 = clean.find({'year':{'$gte': 2009, '$lte':2019}})\n",
    "decade2010 = []\n",
    "\n",
    "for record in words_2010:\n",
    "    decade2010.append(record['text'])\n",
    "\n",
    "type(decade2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop Words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_better = stopwords.words('english')\n",
    "type(stop_words_better)\n",
    "\n",
    "more_stopping = ['applause','said', 'also', 'let']\n",
    "\n",
    "for word in more_stopping:\n",
    "    stop_words_better.append(word)\n",
    "\n",
    "len(stop_words_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12839,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count vectorizer - retrieving words count frequencies.\n",
    "\n",
    "vectorizer=CountVectorizer(stop_words=stop_words_better)\n",
    "X = vectorizer.fit_transform(decade2010)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "#Matrix of word counts for each debate or speech entry. Needs reshaping so word count and vector array are same size.\n",
    "matrix = X.toarray()\n",
    "counts = matrix.sum(axis=0)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.to_json of               Word  Count\n",
      "0               00      6\n",
      "1              000    282\n",
      "2             000k      1\n",
      "3               04      3\n",
      "4               10    135\n",
      "5              100     82\n",
      "6            100th      1\n",
      "7            101st      1\n",
      "8              102      1\n",
      "9              104      2\n",
      "10             105      1\n",
      "11            1099      1\n",
      "12            10th      3\n",
      "13              11     75\n",
      "14             110      1\n",
      "15             112      2\n",
      "16           112th      1\n",
      "17             115      1\n",
      "18             116      2\n",
      "19            11th      6\n",
      "20              12     28\n",
      "21             120      6\n",
      "22           120th      1\n",
      "23             121      1\n",
      "24             122      1\n",
      "25             125      2\n",
      "26            12th      2\n",
      "27              13     30\n",
      "28             135      1\n",
      "29             136      1\n",
      "...            ...    ...\n",
      "12809        yoder      2\n",
      "12810         yoga      1\n",
      "12811         yoke      2\n",
      "12812         york     78\n",
      "12813     yorktown      1\n",
      "12814        young    245\n",
      "12815      younger     12\n",
      "12816     youngest      2\n",
      "12817   youngstown      2\n",
      "12818        youth     14\n",
      "12819     youthful      1\n",
      "12820      youtube      4\n",
      "12821        zabul      1\n",
      "12822         zach      2\n",
      "12823         zags      1\n",
      "12824        zakat      1\n",
      "12825      zardari      1\n",
      "12826     zeitchik      2\n",
      "12827         zero     23\n",
      "12828         zigs      1\n",
      "12829         zika      1\n",
      "12830  zimbabweans      1\n",
      "12831    zimmerman      2\n",
      "12832        zinke      2\n",
      "12833      zionist      2\n",
      "12834          zip      1\n",
      "12835         zone     17\n",
      "12836        zones     11\n",
      "12837       zoning      1\n",
      "12838     zubowski      3\n",
      "\n",
      "[12839 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "#Words and Counts dataframe.\n",
    "df2010 = pd.DataFrame({'word':words, 'count':counts})\n",
    "df2010.sort_values(by='count', ascending=False)\n",
    "\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "json2010 = df.to_json\n",
    "\n",
    "pprint.pprint(json2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.to_json of               Word  Count\n",
      "0               00      6\n",
      "1              000    282\n",
      "2             000k      1\n",
      "3               04      3\n",
      "4               10    135\n",
      "5              100     82\n",
      "6            100th      1\n",
      "7            101st      1\n",
      "8              102      1\n",
      "9              104      2\n",
      "10             105      1\n",
      "11            1099      1\n",
      "12            10th      3\n",
      "13              11     75\n",
      "14             110      1\n",
      "15             112      2\n",
      "16           112th      1\n",
      "17             115      1\n",
      "18             116      2\n",
      "19            11th      6\n",
      "20              12     28\n",
      "21             120      6\n",
      "22           120th      1\n",
      "23             121      1\n",
      "24             122      1\n",
      "25             125      2\n",
      "26            12th      2\n",
      "27              13     30\n",
      "28             135      1\n",
      "29             136      1\n",
      "...            ...    ...\n",
      "12809        yoder      2\n",
      "12810         yoga      1\n",
      "12811         yoke      2\n",
      "12812         york     78\n",
      "12813     yorktown      1\n",
      "12814        young    245\n",
      "12815      younger     12\n",
      "12816     youngest      2\n",
      "12817   youngstown      2\n",
      "12818        youth     14\n",
      "12819     youthful      1\n",
      "12820      youtube      4\n",
      "12821        zabul      1\n",
      "12822         zach      2\n",
      "12823         zags      1\n",
      "12824        zakat      1\n",
      "12825      zardari      1\n",
      "12826     zeitchik      2\n",
      "12827         zero     23\n",
      "12828         zigs      1\n",
      "12829         zika      1\n",
      "12830  zimbabweans      1\n",
      "12831    zimmerman      2\n",
      "12832        zinke      2\n",
      "12833      zionist      2\n",
      "12834          zip      1\n",
      "12835         zone     17\n",
      "12836        zones     11\n",
      "12837       zoning      1\n",
      "12838     zubowski      3\n",
      "\n",
      "[12839 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<WordListCorpusReader in 'C:\\\\Users\\\\madel\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load library\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords)\n",
    "\n",
    "# You will have to download the set of stop words the first time\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'applause',\n",
       " 'applause',\n",
       " 'said',\n",
       " 'also',\n",
       " 'let']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
