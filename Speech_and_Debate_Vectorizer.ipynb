{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "conn = 'mongodb://timanderin.info:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "clean = client.speech_db.clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'debatesCOL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-801acaa8ee22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdebates_coll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdebatesCOL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Pulling the first speech for testing purposes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'debatesCOL' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "debates_coll = debatesCOL.find()\n",
    "\n",
    "#Pulling the first speech for testing purposes.\n",
    "speech = debates_coll[0]['text']\n",
    "\n",
    "#Stripping punctuation.\n",
    "no_punc = speech.translate(str.maketrans('', '', string.punctuation))\n",
    "no_punc\n",
    "\n",
    "#Will append all speeches and debates to list.\n",
    "decades_list = ['what type', 'is this', 'kind of thing', 'kind']\n",
    "decades_list.append(no_punc)\n",
    "len(decades_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decades_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d569f6b9823e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Count vectorizer - retrieving words count frequencies.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecades_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decades_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Count vectorizer - retrieving words count frequencies.\n",
    "vectorizer=CountVectorizer(stop_words= 'english', lowercase=False)\n",
    "X = vectorizer.fit_transform(decades_list)\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "print(len(words))\n",
    "\n",
    "#Matrix of word counts for each debate or speech entry. Needs reshaping so word count and vector array are same size.\n",
    "matrix = X.toarray()\n",
    "counts = matrix.sum(axis=0)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words and Counts dataframe.\n",
    "df = pd.DataFrame({'Word':words, 'Count':counts})\n",
    "df.sort_values(by='Count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2010_count = clean.count_documents({'year':{'$gte': 2009, '$lte':2019}})\n",
    "\n",
    "print(words_2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stuff to import.\n",
    "\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "conn = 'mongodb://timanderin.info:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "clean = client.speech_db.clean\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import json\n",
    "import pprint\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop Words\n",
    "\n",
    "stop_words_better = stopwords.words('english')\n",
    "type(stop_words_better)\n",
    "\n",
    "more_stopping = ['applause','said', 'also', 'let', 'like', 'mr', 'really', 'get']\n",
    "\n",
    "for word in more_stopping:\n",
    "    stop_words_better.append(word)\n",
    "\n",
    "#Count vectorizer - retrieving words count frequencies.\n",
    "stop_words_better = stopwords.words('english')\n",
    "vectorizer=CountVectorizer(stop_words=stop_words_better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Querying the MongoDB.\n",
    "words_2010 = clean.find({'decade':2010})\n",
    "decade2010 = []\n",
    "\n",
    "for record in words_2010:\n",
    "    decade2010.append(record['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11447\n",
      "(11447,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>people</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>applause</td>\n",
       "      <td>1648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4573</th>\n",
       "      <td>going</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11104</th>\n",
       "      <td>want</td>\n",
       "      <td>1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>us</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>one</td>\n",
       "      <td>928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>get</td>\n",
       "      <td>921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2478</th>\n",
       "      <td>country</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5848</th>\n",
       "      <td>know</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>america</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6282</th>\n",
       "      <td>make</td>\n",
       "      <td>855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10611</th>\n",
       "      <td>trump</td>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8951</th>\n",
       "      <td>said</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7808</th>\n",
       "      <td>president</td>\n",
       "      <td>799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>american</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11355</th>\n",
       "      <td>would</td>\n",
       "      <td>751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11406</th>\n",
       "      <td>years</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6016</th>\n",
       "      <td>let</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11340</th>\n",
       "      <td>world</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8779</th>\n",
       "      <td>right</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10313</th>\n",
       "      <td>think</td>\n",
       "      <td>658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10384</th>\n",
       "      <td>time</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>like</td>\n",
       "      <td>650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6887</th>\n",
       "      <td>new</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>clinton</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>jobs</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11208</th>\n",
       "      <td>well</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11328</th>\n",
       "      <td>work</td>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11164</th>\n",
       "      <td>way</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>also</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8104</th>\n",
       "      <td>question</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>better</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>done</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8075</th>\n",
       "      <td>put</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10415</th>\n",
       "      <td>together</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6744</th>\n",
       "      <td>mr</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>job</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7438</th>\n",
       "      <td>percent</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9123</th>\n",
       "      <td>secretary</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6780</th>\n",
       "      <td>must</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>future</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6243</th>\n",
       "      <td>made</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>really</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>donald</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11109</th>\n",
       "      <td>war</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>give</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7424</th>\n",
       "      <td>pence</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>families</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6163</th>\n",
       "      <td>long</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>business</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7586</th>\n",
       "      <td>plan</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10311</th>\n",
       "      <td>thing</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>energy</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11317</th>\n",
       "      <td>women</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6563</th>\n",
       "      <td>million</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10231</th>\n",
       "      <td>tell</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>military</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>ever</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>care</td>\n",
       "      <td>254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2273</th>\n",
       "      <td>congress</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  Count\n",
       "7433      people   1777\n",
       "724     applause   1648\n",
       "4573       going   1295\n",
       "11104       want   1009\n",
       "10906         us    947\n",
       "7089         one    928\n",
       "4521         get    921\n",
       "2478     country    915\n",
       "5848        know    910\n",
       "604      america    887\n",
       "6282        make    855\n",
       "10611      trump    837\n",
       "8951        said    820\n",
       "7808   president    799\n",
       "605     american    785\n",
       "11355      would    751\n",
       "11406      years    740\n",
       "6016         let    704\n",
       "11340      world    686\n",
       "8779       right    662\n",
       "10313      think    658\n",
       "10384       time    654\n",
       "6077        like    650\n",
       "6887         new    648\n",
       "2004     clinton    648\n",
       "5702        jobs    605\n",
       "11208       well    604\n",
       "11328       work    576\n",
       "11164        way    540\n",
       "574         also    522\n",
       "...          ...    ...\n",
       "8104    question    316\n",
       "1209      better    315\n",
       "3261        done    310\n",
       "8075         put    306\n",
       "10415   together    298\n",
       "6744          mr    298\n",
       "5700         job    298\n",
       "7438     percent    297\n",
       "9123   secretary    294\n",
       "6780        must    291\n",
       "4427      future    288\n",
       "6243        made    282\n",
       "8244      really    277\n",
       "3256      donald    276\n",
       "11109        war    275\n",
       "4541        give    272\n",
       "7424       pence    271\n",
       "4001    families    267\n",
       "6163        long    267\n",
       "1574    business    267\n",
       "7586        plan    263\n",
       "10311      thing    262\n",
       "3614      energy    262\n",
       "11317      women    260\n",
       "6563     million    259\n",
       "10231       tell    258\n",
       "6555    military    257\n",
       "3780        ever    254\n",
       "1669        care    254\n",
       "2273    congress    253\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2010\n",
    "\n",
    "X = vectorizer.fit_transform(decade2010)\n",
    "words = vectorizer.get_feature_names()\n",
    "print(len(words))\n",
    "\n",
    "#Matrix of word counts for each debate or speech entry. Needs reshaping so word count and vector array are same size.\n",
    "matrix = X.toarray()\n",
    "counts = matrix.sum(axis=0)\n",
    "print(counts.shape)\n",
    "\n",
    "#Words and Counts dataframe.\n",
    "df2010 = pd.DataFrame({'word':words, 'Count':counts})\n",
    "df2010.sort_values(by='Count', ascending=False).head(100)\n",
    "\n",
    "\n",
    "#json2010 = df2010.to_json\n",
    "\n",
    "#pprint.pprint(json2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1790 = clean.find({'decade':1790})\n",
    "decade1790 = []\n",
    "\n",
    "for record in words_1790:\n",
    "    decade1790.append(record['text'])\n",
    "    \n",
    "# decade1790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4805\n",
      "(4805,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'tag': '15', 'weight': 2}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1790\n",
    "\n",
    "X = vectorizer.fit_transform(decade1790)\n",
    "words = vectorizer.get_feature_names()\n",
    "print(len(words))\n",
    "\n",
    "#Matrix of word counts for each debate or speech entry. Needs reshaping so word count and vector array are same size.\n",
    "matrix = X.toarray()\n",
    "counts = matrix.sum(axis=0)\n",
    "print(counts.shape)\n",
    "\n",
    "\n",
    "#Words and Counts dataframe.\n",
    "\n",
    "#df1790 = pd.DataFrame({'tag':words, 'weight':counts})\n",
    "#df1790.sort_values(by='weight', ascending=False).head(100)\n",
    "#json2010 = df2010.to_json\n",
    "#pprint.pprint(json2010)\n",
    "\n",
    "#d1970 = {}\n",
    "\n",
    "for i in counts:\n",
    "    text = {'tag': words[i], 'weight': counts[i]}\n",
    "    d1970.update(text)\n",
    "\n",
    "d1970\n",
    "\n",
    "#words[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
